{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chEqqJbLzFew"
      },
      "source": [
        "# Yandex Data Science School\n",
        "## Linear Regression & Regularization Exercise.\n",
        "\n",
        "\n",
        "## Outline\n",
        "In this exercise you will learn the following topics:\n",
        "\n",
        "1. Refresher on how linear regression is solved in batch and in Gradient Descent \n",
        "2. Implementation of Ridge Regression\n",
        "3. Comparing Ridge, Lasso and vanila Linear Regression on a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW9qtzOS77sG"
      },
      "source": [
        "# Git Exercise\n",
        "In this exercise you will also experience working with github.\n",
        "\n",
        "You might need to install local python enviroment.\n",
        "Installation Instruction for ex2 - working on a local python environment:\n",
        "https://docs.google.com/document/d/1G0rBo36ff_9JzKy0EkCalK4m_ThNUuJ2bRz463EHK9I\n",
        "\n",
        "## please add the github link of your work below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCRBQ0_V77sK"
      },
      "source": [
        "example: https://github.com/username/exercise_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9UFmk2greT"
      },
      "source": [
        "## Refresher on Ordinary Least Square (OLS) aka Linear Regeression\n",
        "\n",
        "### Lecture Note\n",
        "\n",
        "In Matrix notation, the matrix $X$ is of dimensions $n \\times p$ where each row is an example and each column is a feature dimension. \n",
        "\n",
        "Similarily, $y$ is of dimension $n \\times 1$ and $w$ is of dimensions $p \\times 1$.\n",
        "\n",
        "The model is $\\hat{y}=X\\cdot w$ where we assume for simplicity that $X$'s first columns equals to 1 (one padding), to account for the bias term.\n",
        "\n",
        "Our objective is to optimize the loss $L$ defines as resiudal sum of squares (RSS): \n",
        "\n",
        "$L_{RSS}=\\frac{1}{N}\\left\\Vert Xw-y \\right\\Vert^2$ (notice that in matrix notation this means summing over all examples, so $L$ is scalar.)\n",
        "\n",
        "To find the optimal $w$ one needs to derive the loss with respect to $w$.\n",
        "\n",
        "$\\frac{\\partial{L_{RSS}}}{\\partial{w}}=\\frac{2}{N}X^T(Xw-y)$ (to see why, read about [matrix derivatives](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) or see class notes )\n",
        "\n",
        "Thus, the gardient descent solution is $w'=w-\\alpha \\frac{2}{N}X^T(Xw-y)$.\n",
        "\n",
        "Solving $\\frac{\\partial{L_{RSS}}}{\\partial{w}}=0$ for $w$ one can also get analytical solution:\n",
        "\n",
        "$w_{OLS}=(X^TX)^{-1}X^Ty$\n",
        "\n",
        "The first term, $(X^TX)^{-1}X^T$ is also called the pseudo inverse of $X$.\n",
        "\n",
        "See [lecture note from Stanford](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3MEKz80vdy"
      },
      "source": [
        "## Exercise 1 - Ordinary Least Square\n",
        "* Get the boston housing dataset by using the scikit-learn package. hint: [load_boston](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)\n",
        "\n",
        "* What is $p$? what is $n$ in the above notation? hint: [shape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.shape.html)\n",
        "\n",
        "* write a model `OrdinaryLinearRegression` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score` (which returns the MSE on a given sample set). Hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.\n",
        "\n",
        "* Fit the model. What is the training MSE?\n",
        "\n",
        "* Plot a scatter plot where on x-axis plot $Y$ and in the y-axis $\\hat{Y}_{OLS}$\n",
        "\n",
        "* Split the data to 75% train and 25% test 20 times. What is the average MSE now for train and test? Hint: use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html).\n",
        "\n",
        "* Use a t-test to proove that the MSE for training is significantly smaller than for testing. What is the p-value? Hint: use [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html). \n",
        "\n",
        "* Write a new class `OrdinaryLinearRegressionGradientDescent` which inherits from `OrdinaryLinearRegression` and solves the problem using gradinet descent. The class should get as a parameter the learning rate and number of iteration. Plot the class convergance. What is the effect of learning rate? How would you find number of iteration automatically? Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your features first.\n",
        "\n",
        "* The following parameters are optional (not mandatory to use):\n",
        "    * early_stop - True / False boolean to indicate to stop running when loss stops decaying and False to continue.\n",
        "    * verbose- True/False boolean to turn on / off logging, e.g. print details like iteration number and loss (https://en.wikipedia.org/wiki/Verbose_mode)\n",
        "    * track_loss - True / False boolean when to save loss results to present later in learning curve graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test\n"
          ]
        }
      ],
      "source": [
        "print('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZuSS8LhcfZdn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Ols(object):\n",
        "    \"\"\" An implementation on ordinary least squares\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "\n",
        "    @staticmethod\n",
        "    def pad(X):\n",
        "        X = np.c_[X, np.ones(X.shape[0])]\n",
        "        return X\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # remeber pad with 1 before fitting\n",
        "        # Update the weight\n",
        "        X = Ols.pad(X)\n",
        "        self.w = self._derive(X, Y)\n",
        "\n",
        "    @staticmethod\n",
        "    def _derive(X, Y):\n",
        "        w_ols = np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), Y)\n",
        "        return w_ols\n",
        "\n",
        "    def _fit(self, X, Y):\n",
        "        # optional to use this\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        # return wx\n",
        "        X = Ols.pad(X)\n",
        "        pred_y = np.dot(X, self.w)\n",
        "        return pred_y\n",
        "\n",
        "    def _predict(self, X):\n",
        "        # optional to use this\n",
        "        pass\n",
        "\n",
        "    def score(self, X, Y):\n",
        "        pred_y = self.predict(X)\n",
        "        mse = np.mean((pred_y - Y)**2)\n",
        "        return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            "x: (100, 3)\n",
            "y: (100,)\n",
            "Inputs:\n",
            "x_train: (80, 3)\n",
            "x_test: (20, 3)\n",
            "train_mse: 0.8545201720402403\n",
            "test_mse: 0.9913638336979689\n"
          ]
        }
      ],
      "source": [
        "####### TESTING ########\n",
        "n, k = 100, 2\n",
        "# set the dimensions of the design matrix\n",
        "beta = np.array([1, 1, 10])  # set the true coefficients\n",
        "x = np.concatenate([np.ones((n, 1)), np.random.randn(n, k)], axis=1)  # generate random x\n",
        "y = np.matmul(x, beta) + np.random.randn(n)  # generate random y\n",
        "\n",
        "print('Inputs:')\n",
        "print('x:', x.shape)\n",
        "print('y:', y.shape)\n",
        "\n",
        "frac = int(0.8 * n)\n",
        "train_x = x[:frac,:]\n",
        "train_y = y[:frac]\n",
        "test_x = x[frac:,:]\n",
        "test_y = y[frac:]\n",
        "\n",
        "print('Inputs:')\n",
        "print('x_train:', train_x.shape)\n",
        "print('x_test:', test_x.shape)\n",
        "\n",
        "# ols_obj = Ols()\n",
        "# ols_obj.fit(x, y)\n",
        "# predicted_y = ols_obj.predict(x)\n",
        "# omse = ols_obj.score(x, y)\n",
        "# print('MSE:', omse)\n",
        "\n",
        "ols_obj_t = Ols()\n",
        "ols_obj_t.fit(train_x, train_y)\n",
        "train_mse = ols_obj_t.score(train_x, train_y)\n",
        "test_mse = ols_obj_t.score(test_x, test_y)\n",
        "\n",
        "print('train_mse:', train_mse)\n",
        "print('test_mse:', test_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rx0GRvGk77sV"
      },
      "outputs": [],
      "source": [
        "# Write a new class OlsGd which solves the problem using gradinet descent. \n",
        "# The class should get as a parameter the learning rate and number of iteration. \n",
        "# Plot the loss convergance. for each alpha, learning rate plot the MSE with respect to number of iterations.\n",
        "# What is the effect of learning rate? \n",
        "# How would you find number of iteration automatically? \n",
        "# Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your feature first.\n",
        "class Normalizer():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X):\n",
        "    pass\n",
        "\n",
        "  def predict(self, X):\n",
        "    #apply normalization\n",
        "    pass\n",
        "    \n",
        "class OlsGd(Ols):\n",
        "  \n",
        "  def __init__(self, learning_rate=.05, \n",
        "               num_iteration=1000, \n",
        "               normalize=True,\n",
        "               early_stop=True,\n",
        "               verbose=True):\n",
        "    \n",
        "    super(OlsGd, self).__init__()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iteration = num_iteration\n",
        "    self.early_stop = early_stop\n",
        "    self.normalize = normalize\n",
        "    self.normalizer = Normalizer()    \n",
        "    self.verbose = verbose\n",
        "    \n",
        "  def _fit(self, X, Y, reset=True, track_loss=True):\n",
        "    #remeber to normalize the data before starting\n",
        "    pass\n",
        "        \n",
        "  def _predict(self, X):\n",
        "    #remeber to normalize the data before starting\n",
        "    pass\n",
        "      \n",
        "  def _step(self, X, Y):\n",
        "    # use w update for gradient descent\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HVfnXvZFi98"
      },
      "source": [
        "## Exercise 2 - Ridge Linear Regression\n",
        "\n",
        "Recall that ridge regression is identical to OLS but with a L2 penalty over the weights:\n",
        "\n",
        "$L(y,\\hat{y})=\\sum_{i=1}^{i=N}{(y^{(i)}-\\hat{y}^{(i)})^2} + \\lambda \\left\\Vert w \\right\\Vert_2^2$\n",
        "\n",
        "where $y^{(i)}$ is the **true** value and $\\hat{y}^{(i)}$ is the **predicted** value of the $i_{th}$ example, and $N$ is the number of examples\n",
        "\n",
        "* Show, by differentiating the above loss, that the analytical solution is $w_{Ridge}=(X^TX+\\lambda I)^{-1}X^Ty$\n",
        "* Change `OrdinaryLinearRegression` and `OrdinaryLinearRegressionGradientDescent` classes to work also for ridge regression (do not use the random noise analogy but use the analytical derivation). Either add a parameter, or use inheritance.\n",
        "* **Bonus: Noise as a regularizer**: Show that OLS (ordinary least square), if one adds multiplicative noise to the features the **average** solution for $W$ is equivalent to Ridge regression. In other words, if $X'= X*G$ where $G$ is an uncorrelated noise with variance $\\sigma$ and mean 1, then solving for $X'$ with OLS is like solving Ridge for $X$. What is the interpretation? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JKrcabv777sY"
      },
      "outputs": [],
      "source": [
        "class RidgeLs(Ols):\n",
        "  def __init__(self, ridge_lambda, *wargs, **kwargs):\n",
        "    super(RidgeLs,self).__init__(*wargs, **kwargs)\n",
        "    self.ridge_lambda = ridge_lambda\n",
        "    \n",
        "  def _fit(self, X, Y):\n",
        "    #Closed form of ridge regression\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voV47cV577se"
      },
      "source": [
        "### Use scikitlearn implementation for OLS, Ridge and Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtdXmQ4T77sf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "d207591e6ff77a7c5fb4cef0dd9fd3703274637a9d0902d2045beb3a65bf572a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
